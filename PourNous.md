# Introduction

Ce fichier n'est pas destinÃ© Ã  Ãªtre push sur le repo, il est juste lÃ  pour que nous comprenions ce que nous faisons.

## Ã‰tape 1 â€“ MLP sur donnÃ©es brutes

Fichier : `MLP_from_raw_data.ipynb`  
**But** : ImplÃ©menter et comparer plusieurs rÃ©seaux MLP classiques (1 couche cachÃ©e), entraÃ®nÃ©s sur les pixels bruts du jeu de donnÃ©es MNIST.

### ğŸ¯ Objectif global de l'Ã©tape 1

Lâ€™objectif est de classer des images de chiffres manuscrits (`MNIST`) en utilisant un rÃ©seau de neurones multicouches (`MLP`). On commence avec des donnÃ©es "brutes", câ€™est-Ã -dire les pixels directement, sans traitement prÃ©alable.

### ğŸ”¢ Pourquoi utiliser les pixels comme entrÃ©e ?

- Une image `MNIST` est composÃ©e de `28x28 = 784 pixels`, en niveaux de gris.
- Chaque pixel contient une information importante : câ€™est la reprÃ©sentation directe du chiffre.
- Utiliser les pixels bruts permet de tester la capacitÃ© du rÃ©seau Ã  apprendre sans aide extÃ©rieure (pas d'extraction manuelle de caractÃ©ristiques).

### ğŸ§  Pourquoi un `MLP` (Multilayer Perceptron) ?

- Le `MLP` est une architecture simple et standard en deep learning, utile pour Ã©tablir une base de comparaison.
- Il est dit â€œshallowâ€ ici car il ne contient quâ€™une seule couche cachÃ©e.
- Il permet dâ€™apprendre des relations non linÃ©aires entre pixels et classes (0 Ã  9).

### ğŸ—ï¸ Pourquoi tester plusieurs tailles de couches cachÃ©es (128, 256, 512) ?

- Une plus grande couche cachÃ©e permet de mieux approximer des fonctions complexesâ€¦ mais :
    - demande plus de temps d'entraÃ®nement.
    - augmente le risque de surapprentissage (`overfitting`).
- Tester diffÃ©rentes tailles permet dâ€™Ã©quilibrer capacitÃ© de gÃ©nÃ©ralisation et complexitÃ©.

### âš™ï¸ Pourquoi utiliser softmax en sortie ?

- Il s'agit d'une classification multiclasse (10 classes).
- La couche softmax retourne une probabilitÃ© par classe, ce qui permet dâ€™interprÃ©ter la sortie comme une prÃ©diction.

### ğŸ§® Pourquoi categorical_crossentropy comme fonction de perte ?

- Câ€™est la fonction de perte adaptÃ©e aux problÃ¨mes de classification multiclasse avec one-hot encoding.
- Elle mesure l'Ã©cart entre la distribution prÃ©dite (softmax) et la vÃ©ritÃ© terrain (one-hot).

### ğŸ“ˆ Pourquoi visualiser accuracy et courbe de perte ?

Cela permet de :

- vÃ©rifier si le modÃ¨le apprend bien (val_accuracy â†‘, val_loss â†“),
- dÃ©tecter un overfitting (val_accuracy stagne ou diminue alors que train_accuracy â†‘).

### ğŸ” Pourquoi une matrice de confusion ?

- Elle permet d'analyser finement les erreurs du modÃ¨le.
- Elle montre quelles classes sont confondues entre elles, ce qui Ã©claire les limites du modÃ¨le.

## Ã‰tape 2 â€“ MLP avec extraction HOG

Fichier : `MLP_from_HOG.ipynb`  
**But** : Comparer lâ€™approche "pixels bruts" avec une approche oÃ¹ les images sont d'abord transformÃ©es en vecteurs de caractÃ©ristiques HOG, puis classÃ©es avec un MLP.

### ğŸ¯ Objectif global de l'Ã©tape 2

Lâ€™objectif est toujours de classer des chiffres manuscrits (`MNIST`) mais cette fois-ci **en extrayant des caractÃ©ristiques (features)** Ã  lâ€™aide de `HOG` avant dâ€™entraÃ®ner un `MLP`.
On cherche Ã  comparer lâ€™approche *brute* (pixels) avec une approche *basÃ©e sur des descripteurs visuels*.

---

### ğŸ§ª Pourquoi utiliser `HOG` (Histogram of Oriented Gradients) ?

- HOG est une mÃ©thode classique dâ€™extraction de caractÃ©ristiques visuelles.
- Elle dÃ©crit les **bords et contours** prÃ©sents dans une image.
- Elle est plus **invariante aux variations locales** (bruit, petites dÃ©formations) que les pixels bruts.
- Elle permet au MLP dâ€™apprendre sur des **informations plus compactes et discriminantes**.

---

### âš™ï¸ Pourquoi tester plusieurs `pix_per_cell` (4 et 7) ?

- Ce paramÃ¨tre dÃ©termine la **rÃ©solution locale de dÃ©tection des gradients** :
  - Une petite cellule (4x4) capte plus de dÃ©tails.
  - Une grande cellule (7x7) donne une reprÃ©sentation plus globale.
- Comparer ces deux tailles permet de **mesurer lâ€™impact de la granularitÃ© des descripteurs** sur les performances du rÃ©seau.

---

### ğŸ”¢ Pourquoi standardiser les features (`StandardScaler`) ?

- Les valeurs de HOG peuvent varier selon lâ€™intensitÃ© des pixels et la taille des cellules.
- La standardisation (moyenne = 0, Ã©cart-type = 1) :
  - accÃ©lÃ¨re lâ€™apprentissage,
  - stabilise lâ€™optimiseur,
  - empÃªche quâ€™une dimension domine les autres.

---

### ğŸ§  Pourquoi encore un `MLP` ?

- Le but est de comparer les mÃªmes types de modÃ¨les (**shallow MLP**) mais avec des **entrÃ©es diffÃ©rentes** (pixels bruts vs. HOG).
- Cela permet de voir si lâ€™extraction de features amÃ©liore ou dÃ©tÃ©riore la performance dâ€™un rÃ©seau simple.

---

### ğŸ—ï¸ Pourquoi tester plusieurs tailles de couches cachÃ©es (128, 256, 512) ?

- MÃªme logique que pour lâ€™Ã©tape 1 : tester la capacitÃ© dâ€™approximation du rÃ©seau en fonction du nombre de neurones.

---

### ğŸ“Š Pourquoi comparer les courbes et matrices de confusion ?

- Comme Ã  lâ€™Ã©tape 1, cela permet de :
  - suivre la **convergence de lâ€™apprentissage**,
  - identifier les **chiffres mal classÃ©s** malgrÃ© les features HOG,
  - **comparer objectivement avec les rÃ©sultats obtenus avec les pixels bruts**.

---

### âš–ï¸ Conclusion

Cette Ã©tape permet de comprendre :

- Lâ€™intÃ©rÃªt dâ€™utiliser des descripteurs (comme HOG) en entrÃ©e dâ€™un rÃ©seau,
- Les impacts des choix de paramÃ¨tres (cellule, neurones),
- Comment lâ€™extraction de features modifie la performance dâ€™un rÃ©seau simple.

## Ã‰tape 3 â€“ CNN

Fichier : `CNN.ipynb`  
**But** : Utiliser des rÃ©seaux convolutifs (CNN) pour amÃ©liorer les performances de classification sur MNIST, en explorant plusieurs architectures (filtres, dropout, etc.).

### ğŸ¯ Objectif global de l'Ã©tape 3

L'objectif est de construire et d'entraÃ®ner plusieurs modÃ¨les CNN, en faisant varier les **paramÃ¨tres structurels** comme le nombre de filtres, la taille des noyaux, la prÃ©sence de Dropout, etc.  
Le but final est de comparer la **puissance des CNN** face aux MLP, et d'Ã©tudier lâ€™impact de certains choix dâ€™architecture.

---

### ğŸ§  Pourquoi un `CNN` (Convolutional Neural Network) ?

- Contrairement aux MLP, les CNN exploitent la **structure spatiale des images**.
- Ils utilisent des **filtres convolutifs** qui dÃ©tectent des motifs (bords, textures, formes).
- Ils nÃ©cessitent **beaucoup moins de paramÃ¨tres** pour des rÃ©sultats souvent bien meilleurs.

---

### ğŸ” Pourquoi tester plusieurs combinaisons (filtres, tailles, dropout) ?

- Chaque paramÃ¨tre modifie la **capacitÃ© dâ€™extraction des motifs visuels** :
  - **Plus de filtres** â†’ plus de diversitÃ© de motifs dÃ©tectÃ©s.
  - **Plus grand noyau (5x5)** â†’ motifs plus larges (mais plus coÃ»teux).
  - **Dropout** â†’ rÃ©duction du surapprentissage.
- En testant **8 combinaisons**, on peut :
  - **mesurer l'impact de chaque paramÃ¨tre**,
  - **identifier la meilleure configuration**,
  - conclure objectivement sur leur utilitÃ©.

---

### ğŸ“ˆ Pourquoi visualiser les courbes de validation ?

- Elles permettent de suivre **l'Ã©volution de la prÃ©cision sur les donnÃ©es non vues**.
- Une courbe stable qui monte suggÃ¨re un bon apprentissage.
- On peut identifier si un modÃ¨le **sur-apprend** (Ã©cart train/val) ou **sous-apprend**.

---

### ğŸ” Pourquoi une matrice de confusion finale ?

- MÃªme logique que pour les Ã©tapes 1 et 2.
- Ici elle confirme que le CNN **fait moins dâ€™erreurs de confusion** que les autres modÃ¨les.

---

### ğŸ§ª Quel est le meilleur modÃ¨le et pourquoi ?

- Le modÃ¨le **F32_K5_DO_D128** est le meilleur (99.05% de prÃ©cision test).
- Il utilise :
  - **32 filtres**
  - **noyaux 5x5**
  - **Dropout** (25% aprÃ¨s conv, 50% aprÃ¨s dense)
- Il montre que :
  - le **dropout aide clairement** Ã  la gÃ©nÃ©ralisation,
  - un modÃ¨le modeste mais bien rÃ©gularisÃ© peut surpasser des architectures plus grandes.

---

### âš–ï¸ Comparaison avec les MLP prÃ©cÃ©dents

| ModÃ¨le        | Accuracy test |
|---------------|---------------|
| MLP (raw)     | 98.26%        |
| MLP (HOG)     | 98.40%        |
| CNN (meilleur)| **99.05%**    |

Le CNN surpasse clairement les autres modÃ¨les, tout en conservant une structure assez simple.  
Cela confirme que les architectures convolutives sont **mieux adaptÃ©es Ã  lâ€™analyse dâ€™images**, mÃªme simples comme MNIST.

## Ã‰tape 4 â€“ CNN pour la dÃ©tection de pneumonie

Fichierâ€¯: `CNN_pneumonia.ipynb`  
**But**â€¯: Construire un CNN binaire (pneumonieâ€¯/â€¯normal) Ã  partir de radiographies pulmonaires en niveaux de gris.

### ğŸ¯ Objectif global de l'Ã©tapeâ€¯4
DÃ©tecter automatiquement la pneumonie sur des clichÃ©s thoraciques afin de dÃ©montrer la transfÃ©rabilitÃ© des CNN Ã  un **domaine mÃ©dical plus complexe** que MNIST.

---

### ğŸ§  Pourquoi un `CNN` pour les radios pulmonairesâ€¯?
- Les radiographies contiennent des **patrons visuels locaux** (opacitÃ©s, infiltrats) que les filtres convolutifs identifient mieux quâ€™un MLP.  
- Le CNN gÃ¨re les **variations dâ€™Ã©chelle et de position** grÃ¢ce au pooling.  
- Le nombre de paramÃ¨tres reste raisonnable par rapport Ã  un rÃ©seau entiÃ¨rement dense sur des images 150Ã—150.

---

### ğŸ—ï¸ Architecture retenue
1. **Convâ€¯32â€¯(3Ã—3) â†’ ReLU â†’ MaxPool**  
2. **Convâ€¯64â€¯(3Ã—3) â†’ ReLU â†’ MaxPool**  
3. **Convâ€¯128â€¯(3Ã—3) â†’ ReLU â†’ MaxPool**  
4. **Dropoutâ€¯0.5**  
5. **Flatten â†’ Denseâ€¯128â€¯(ReLU) â†’ Denseâ€¯1â€¯(sigmoÃ¯de)**  

*Hyperâ€‘paramÃ¨tres*â€¯: Adam, `binary_crossentropy`, batchâ€¯32, 10â€¯Ã©poques (earlyâ€‘stopping activÃ©).

---

### âš™ï¸ Pourquoi ces choixâ€¯?
- **Images 150Ã—150 / niveaux de gris**â€¯: compromis entre dÃ©finition et mÃ©moire GPU.  
- **SigmoÃ¯de + binary_crossentropy**â€¯: adaptÃ© Ã  une classification **binaire**.  
- **Dropout 0.5**â€¯: essentiel pour **rÃ©duire le surapprentissage** sur un dataset limitÃ©.  
- **Recall priorisÃ©**â€¯: rater une pneumonie est plus grave quâ€™un faux positif.

---

### ğŸ“ˆ RÃ©sultats principaux
| Classe        | PrÃ©cision | Rappel | F1â€‘score |
|---------------|-----------|--------|----------|
| Pneumonia     | 0.81      | **0.99** | 0.88     |
| Normal        | **0.95**  | 0.58   | 0.73     |

- **Accuracy globale**â€¯: 83.8â€¯% (sur le jeu de test).  
- **Matrice de confusion**â€¯: trÃ¨s peu de faux nÃ©gatifs, mais un nombre notable de faux positifs (normaux âœ pneumonie).

---

### ğŸ” Analyse des performances
- **SensibilitÃ© Ã©levÃ©e** (rappel 0.99)â€¯: le modÃ¨le repÃ¨re quasi toutes les pneumonies, ce qui est primordial en clinique.  
- **SpÃ©cificitÃ© limitÃ©e**â€¯: beaucoup de normaux incorrectement classÃ©s âœ besoin de rÃ©gularisation ou de rÃ©Ã©quilibrage.  
- **DÃ©sÃ©quilibre de classes**â€¯: le dataset contient ~3â€¯Ã— plus dâ€™images de pneumonie que de normales, dâ€™oÃ¹ le biais.

---

### ğŸ› ï¸ Pistes dâ€™amÃ©lioration
- **Data augmentation ciblÃ©e** (rotations lÃ©gÃ¨res, translations) pour enrichir la classe *Normal*.  
- **RÃ©Ã©chantillonnage ou pondÃ©ration des classes** dans la fonction de perte.  
- **Batch Normalization** entre convolutions pour stabiliser lâ€™apprentissage.  
- **EntraÃ®nement plus long + earlyâ€‘stopping** afin dâ€™atteindre un meilleur point de validation.

---

### âš–ï¸ Conclusion
Le CNN dÃ©tecte efficacement la pneumonie (trÃ¨s peu de faux nÃ©gatifs) mais doit rÃ©duire les faux positifs avant toute application clinique rÃ©elle. Des techniques de rÃ©â€‘Ã©quilibrage et de rÃ©gularisation devraient permettre dâ€™amÃ©liorer la **spÃ©cificitÃ©** sans sacrifier la **sensibilitÃ©**.
